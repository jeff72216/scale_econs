"""
          Economies of Scale to Consumption in Collective Households
              Hsin-Yuan Hsieh, Arthur Lewbel, & Krishna Pendakur
                     Bootstrap (clustered, without LASSO)
"""
import os
import pyreadstat
import numpy as np
import pandas as pd

from multiprocessing import Pool
from scipy.sparse import csr_array
from linearmodels.system import SUR
from collections import OrderedDict
from tqdm import tqdm

from bocpdms.nearestPD import NPD # Borrowed from https://github.com/alan-turing-institute/bocpdms.git


# Define functions.
def demean(var, gt):
    """Difference-out the group mean from each observation.

    Parameters:
    -----------
    var: 
        An N*P NumPy array of the sample where N is the number of observations  
        and P is the number of variables.
    gt:
        A sparse matrix of group indicators generated by the function 
        scipy.sparse.csr_array(matrix). The input matrix must be an N*G matrix 
        where N is the number of observations and G is the number of groups.

    Outputs:
    --------
    var_demean: 
        An N*P NumPy array of the demeaned sample where N is the number of      
        observations and P is the number of variables.
    """
    var_demean = np.zeros((var.shape[0], var.shape[1]))

    for i in np.unique(gt[1]):
        index = gt[0][np.where(gt[1] == i)]
        var_demean[index, :] = var[index, :] - var[index, :].mean(axis=0)
    
    return var_demean

def bootstrap(
        resampling, 
        shs_data, 
        demean = demean, 
        TYPE=(24, 27, 28), 
        SHAREABLE=(1, 2, 8, 17, 19), 
        NONSHAREABLE=6, 
        DEMOG=(1, 2)):
    # Generate a bootstrap sample.
    shs_data = pd.DataFrame(
        shs_data, 
        columns=([f's{i}' for i in SHAREABLE] 
                 + [f's{NONSHAREABLE}'] 
                 + [f'p{i}' for i in SHAREABLE] 
                 + [f'p{NONSHAREABLE}'] 
                 + ['x'] 
                 + [f'z{i}' for i in DEMOG] 
                 + [f'z{i}' for i in (3, 23)] 
                 + [f'z{i}' for i in TYPE] 
                 + [f'z{i}' for i in range(29, 51)])
        )
    shs_data = shs_data.loc[resampling]


    # Setup groups (province-year-type).
    g = pd.DataFrame(
        (shs_data[
            [f'z{i}' for i in range(38, 51)]
            ].values[:, None] 
         * shs_data[
             [f'z{j}' for j in range(29, 38)]
             ].values[..., None]).reshape(shs_data.shape[0], -1)
        )
    NAME_g = [f'g{i+1}' for i in range(np.shape(g)[1])]
    g.columns = NAME_g

    sm = np.where((shs_data['z23'] == 1) & (shs_data['z3'] == 0), 1, 0)
    sf = np.where((shs_data['z23'] == 1) & (shs_data['z3'] == 1), 1, 0)
    t = np.vstack((sm, sf)).T

    for i in TYPE:
        t = np.hstack((t, np.where(shs_data[f'z{i}'] == 1, 1, 0)[:, None]))

    gt = pd.DataFrame(
        (g.values[:, None] * t[..., None]).reshape(shs_data.shape[0], -1), 
        index = shs_data.index
        )
    gt_excluded = [
        i for i in range(gt.shape[1]) if len(
            np.unique(gt[i][gt[i] == 1].index)
            ) == 1
        ]
    gt = gt.reset_index(drop=True)
    idx_excluded = sum(
        [gt.index[gt[i] == 1].tolist() for i in gt_excluded], []
        )
    
    shs_data = (
        shs_data.reset_index(drop=True)
        ).drop(idx_excluded).reset_index(drop=True)
    g = g.drop(idx_excluded).reset_index(drop=True)
    t = np.delete(t, idx_excluded, 0)
    NAME_t = ['sm', 'sf'] + [f'h{i+1}' for i in range(len(TYPE))]
    t_single = np.hstack(((t[:, 0]+t[:, 1])[:, None], t[:, 2:]))
    NAME_t_single = ['s'] + [f'h{i+1}' for i in range(len(TYPE))]

    gt = csr_array(
        gt.drop(idx_excluded).reset_index(drop=True).values
        ).nonzero()
    

    # Create variables.
    y = np.log(shs_data['x'].values) - shs_data[f'p{NONSHAREABLE}'].values
    r = ((shs_data[[f'p{i}' for i in SHAREABLE]]).values 
         - shs_data[f'p{NONSHAREABLE}'].values[:, None])
    er = np.exp(r)
    z = shs_data[[f'z{i}' for i in DEMOG]].values
    w = ((shs_data[[f's{i}' for i in SHAREABLE]]).values 
         / shs_data['x'].values[:, None])
    w_non = shs_data[f's{NONSHAREABLE}'].values / shs_data['x'].values
    NAME_w = [f'w{i}' for i in SHAREABLE]
    wyz = np.hstack((w, y[:, None], z))
    wyz_demean = demean(wyz, gt)
    ery = er * wyz_demean[:, len(SHAREABLE)][:, None]


    # Group variables by types.
    eryt = pd.DataFrame(
        (ery[:, None] * t_single[..., None]).reshape(t_single.shape[0], -1), 
        columns=[f'er{SHAREABLE[i]}y_{tt}'
                 for tt in NAME_t_single 
                 for i in range(len(SHAREABLE))]
        )
    NAME_eryt = eryt.columns.tolist()
    zt = pd.DataFrame(
        (wyz_demean[:, (len(SHAREABLE)+1):][:, None]
         * t[..., None]).reshape(t.shape[0], -1), 
         columns=[f'z{DEMOG[i]}_{tt}'
                  for tt in NAME_t 
                  for i in range(len(DEMOG))]
        )
    NAME_zt = zt.columns.tolist()


    # Combine variables into a single dataframe.
    shs_data = pd.concat([
        pd.DataFrame(
            wyz_demean[:, range(len(SHAREABLE))], 
            columns = NAME_w
            ), 
        eryt, 
        zt
        ], axis=1)


    # Estimate reduced-form equations (mean Barten scales).
    mean_boot_eqs = OrderedDict()

    for i in range(len(SHAREABLE)):
        dep = shs_data[NAME_w[i]]
        exog = shs_data[
            [col for col in NAME_eryt if f"er{SHAREABLE[i]}y_" in col] 
            + NAME_zt
            ]
        mean_boot_eqs[f'eq{i+1}'] = {"dependent":dep, "exog":exog}

    mean_boot_est = SUR(mean_boot_eqs).fit(cov_type="unadjusted")


    # Prepare data for the estimation of Barten scale variances.
    ery = er * y[:, None]
    eryt = (ery[:,None] * t_single[...,None]).reshape(t_single.shape[0], -1)
    zt = (z[:,None] * t[...,None]).reshape(t.shape[0], -1)
    res = np.zeros((np.shape(shs_data)[0], len(SHAREABLE)))

    for i in range(len(SHAREABLE)):
        fres = w[:, i] - (
            np.hstack(
                (eryt[:, np.arange(i, len(NAME_eryt), len(SHAREABLE))], zt)
                )
            @ mean_boot_est.params.filter(like=f'eq{i+1}').values[:, None]
            ).T.flatten()
        for j in np.unique(gt[1]):
            index = gt[0][np.where(gt[1] == j)]
            res[index, i] = fres[index] - np.mean(fres[index])

    bery = ery * mean_boot_est.params.filter(like=f'y_s').values
    bery_crossprod = pd.DataFrame(index=shs_data.index)
    bery_crosssum = pd.DataFrame(index=shs_data.index)
    res_crossprod = pd.DataFrame(index=shs_data.index)

    for i in range(len(SHAREABLE)):
        for j in range(i, len(SHAREABLE)):
            res_crossprod = pd.concat(
                [res_crossprod, 
                 pd.DataFrame(
                     res[:, i] * res[:, j], 
                     columns=[f'res{SHAREABLE[i]}.{SHAREABLE[j]}']
                     )], 
                axis=1
                )
            bery_crossprod = pd.concat(
                [bery_crossprod, 
                 pd.DataFrame(
                     bery[:, i] * bery[:, j], 
                     columns=[f'ber{SHAREABLE[i]}.{SHAREABLE[j]}y_prod']
                     )], 
                axis=1
                )
            bery_crosssum = pd.concat(
                [bery_crosssum, 
                 pd.DataFrame(
                     bery[:, i] + bery[:, j], 
                     columns=[f'ber{SHAREABLE[i]}.{SHAREABLE[j]}y_sum']
                     )], 
                axis=1
                )
        
    NAME_res_crossprod = res_crossprod.columns
    NAME_bery_crossprod = bery_crossprod.columns
    NAME_bery_crosssum = bery_crosssum.columns

    shs_boot_res = pd.concat([
        pd.DataFrame(t_single[:, 1:], columns=NAME_t[2:]), 
        res_crossprod, 
        bery_crossprod, 
        g, 
        pd.DataFrame(
            (bery_crosssum.values[:,None]
             * g.values[...,None]).reshape(g.shape[0], -1),
            columns=[f'g{i+1}_{NAME_bery_crosssum[j]}'
                     for i in range(np.shape(g)[1])
                     for j in range(len(NAME_bery_crosssum))]
            )
        ], 
        axis=1)[~(t_single[:, 0] == 1)].reset_index(drop=True)


    # Estimate Barten scale variances.
    for tt in range(len(TYPE)):
        res_boot = shs_boot_res[shs_boot_res[f'h{tt+1}'] == 1].astype(float)
        g_included_boot = [i for i in NAME_g if res_boot[i].sum() >= 2]
        g_bery_included_boot = [
            f'{i}_{j}' for i in g_included_boot for j in NAME_bery_crosssum
            ]
        g_new_boot = csr_array(res_boot[NAME_g].values)
        g_new_boot = g_new_boot.nonzero()
        res_boot = pd.DataFrame(demean(res_boot.values, g_new_boot), 
                                columns=res_boot.columns)
        var_eqs_boot = OrderedDict()
        for i in range(len(NAME_res_crossprod)):
            dep = res_boot[NAME_res_crossprod[i]]
            name = NAME_res_crossprod[i].replace("res", "ber") + 'y'
            exog = res_boot[[j for j in g_bery_included_boot if name in j] 
                            + [NAME_bery_crossprod[i]]]
            
            var_eqs_boot[f'eq{i+1}'] = {"dependent": dep, "exog": exog}

        cov_est_boot = SUR(var_eqs_boot).fit(cov_type="unadjusted")
        
        cov_val_new = cov_est_boot.params[
            cov_est_boot.params.index.str.contains("prod")
            ]
        
        cov = np.empty((len(SHAREABLE), len(SHAREABLE)))
        cov[np.triu_indices_from(cov, k=0)] = cov_val_new
        cov[np.tril_indices_from(cov, k=-1)] = cov.T[
            np.tril_indices_from(cov, k=-1)
            ]

        cov_pd = NPD.nearestPD(cov)
        std_new = pd.Series(np.sqrt(np.diag(cov_pd)))
        std = (std_new.copy() if tt==0 else pd.concat([std, std_new]))

    return std

def bootstrap_star(args):
    return bootstrap(*args)


# Computation starts.
if __name__ == '__main__':
    # Define parameters.
    AGE = 65 # Age limit
    BOUND = (0.05, 0.90) # (lower, upper) Bounds of expenditure percentile
    TYPE = (24, 27, 28) # Household types
    NUM_PEOPLE = (2, 3, 4) # Number of people in a household
    SHAREABLE = (1, 2, 8, 17, 19) # Shareable goods
    NONSHAREABLE = 6 # Nonshareable good
    DEMOG = (1, 2) # Demographic variables
    rep = 1000 # Number of replications in bootstrap
    np.random.seed(123)


    # Read and filter raw data, exclude observations:
    # 1. in cities under 100k population (z21),
    # 2. in rural area (z22), 
    # 3. above the age limit (z51).
    os.chdir("/Users/Apple/Desktop/Research/econs_scale/data/Canada_SHS/97-09")
    shs_data, meta = pyreadstat.read_dta("SHS97to09_tenants_new.dta")
    shs_data = shs_data[
        ((shs_data[['z23'] + [f'z{idx}' for idx in TYPE]].sum(axis=1) == 1) & 
         (shs_data[['z21', 'z22']].sum(axis=1) == 0) & 
         (shs_data['z51'] <= AGE))
        ]
    

    # Remove observations with missing values.
    for i in SHAREABLE:
        shs_data = shs_data[(shs_data[f's{i}'] != 0)]
    shs_data = shs_data[(shs_data[f's{NONSHAREABLE}'] != 0)]


    # Exclude observations outside the bounds of expenditure percentile.
    shs_data['x'] = shs_data[[f's{i}' for i in SHAREABLE] 
                             + [f's{NONSHAREABLE}']].sum(axis=1)
    selected = (
        (shs_data['x'] > np.quantile(shs_data['x'], BOUND[0])) &
        (shs_data['x'] < np.quantile(shs_data['x'], BOUND[1]))
        )
    shs_data = shs_data[selected].reset_index(drop=True)


    # Generate a new sample for bootstrap.
    shs_boot_cols = ([f's{i}' for i in SHAREABLE] 
                     + [f's{NONSHAREABLE}'] 
                     + [f'p{i}' for i in SHAREABLE] 
                     + [f'p{NONSHAREABLE}'] 
                     + ['x'] 
                     + [f'z{i}' for i in DEMOG] 
                     + [f'z{i}' for i in (3, 23)] 
                     + [f'z{i}' for i in TYPE] 
                     + [f'z{i}' for i in range(29, 51)])
    shs_boot = shs_data[shs_boot_cols].to_numpy()


    # Setup groups (province-year-type).
    g = pd.DataFrame(
        (shs_data[
            [f'z{i}' for i in range(38, 51)]
            ].values[:, None] 
         * shs_data[
             [f'z{j}' for j in range(29, 38)]
             ].values[..., None]).reshape(shs_data.shape[0], -1)
        )
    NAME_g = [f'g{i+1}' for i in range(np.shape(g)[1])]
    g.columns = NAME_g

    clusters = [
        int(
            g.columns[g.iloc[i].isin([1.0])][0].replace('g', '')
            ) for i in range(g.shape[0])
            ]


    # Generate an 2-d array of indices for the resampling process.
    resampling = np.random.choice(
        range(clusters.index(list(set(clusters))[1])), 
        size=(rep, clusters.index(list(set(clusters))[1]))
        )
    for i in range(1, len(list(set(clusters)))-1):
        resampling_new = np.random.choice(
            range(clusters.index(list(set(clusters))[i]),
                  clusters.index(list(set(clusters))[i+1])), 
            size=(
                rep, 
                (clusters.index(list(set(clusters))[i+1])
                 -clusters.index(list(set(clusters))[i]))
                )
            )
        resampling = np.append(resampling, resampling_new, axis=1)
    resampling_new = np.random.choice(
        range(clusters.index(list(set(clusters))[len(list(set(clusters)))-1]), 
              shs_boot.shape[0]), 
        size=(
            rep, 
            (shs_boot.shape[0]
             -clusters.index(list(set(clusters))[len(list(set(clusters)))-1]))
            )
        )
    resampling = np.append(resampling, resampling_new, axis = 1)


    # Bootstrap starts, use parallel process.
    args = [(resampling[iter], shs_boot) for iter in range(rep)]
    pool = Pool()
    results_store = list(tqdm(pool.imap(bootstrap_star, args), total=rep))
    pool.close()
    pool.join()


    # Translate the output of the bootstrap function into a variable.
    std_store = np.zeros((rep, int(len(SHAREABLE)*len(TYPE))))
    for i in range(rep):
        std_store[i, :] = results_store[i].to_numpy()


    # Calculate bootstrap standard errors.
    std_se_df = pd.DataFrame(
        np.std(std_store, axis=0).reshape([len(TYPE), len(SHAREABLE)]).T, 
        index=[f'w{i}' for i in SHAREABLE], 
        columns=[f'std.se.h{i}' for i in range(len(TYPE))], 
        dtype=float
        )

    # Print results.
    np.set_printoptions(precision=4, suppress=True)
    print("standard deviation standard errors")
    print(std_se_df.astype(float).round(4))