"""
          Economies of Scale to Consumption in Collective Households
              Hsin-Yuan Hsieh, Arthur Lewbel, & Krishna Pendakur
                      Estimation (clustered, with LASSO)
"""
import os
import pyreadstat
import linearmodels
import scipy
import numpy as np
import pandas as pd

from linearmodels.system import SUR
from collections import OrderedDict
from sklearn.model_selection import RepeatedKFold

from bocpdms.nearestPD import NPD # Borrowed from https://github.com/alan-turing-institute/bocpdms.git
from custom_enet import CustomENetCV, CustomENet # Borrowed from https://github.com/3zhang/Python-Lasso-ElasticNet-Ridge-Regression-with-Customized-Penalties.git


# Define global parameters.
AGE = 65 # Age limit
BOUND = (0.05, 0.90) # (lower, upper) Bounds of expenditure percentile
TYPE = (24, 27, 28) # Household types
NUM_PEOPLE = (2, 3, 4) # Number of people in a household
SHAREABLE = (1, 2, 8, 17, 19) # Shareable goods
NONSHAREABLE = 6 # Nonshareable good
DEMOG = (1, 2) # Demographic variables


# Read and filter raw data, exclude observations:
# 1. in cities under 100k population (z21),
# 2. in rural area (z22), 
# 3. above the age limit (z51).
os.chdir("/Users/Apple/Desktop/Research/econs_scale/data/Canada_SHS/97-09")
shs_data, meta = pyreadstat.read_dta("SHS97to09_tenants_new.dta")
shs_data = shs_data[
    ((shs_data[['z23'] + [f'z{idx}' for idx in TYPE]].sum(axis=1) == 1) & 
     (shs_data[['z21', 'z22']].sum(axis=1) == 0) & 
     (shs_data['z51'] <= AGE))
    ]


# Remove observations with missing values.
for i in SHAREABLE:
    shs_data = shs_data[(shs_data[f's{i}'] != 0)]
shs_data = shs_data[(shs_data[f's{NONSHAREABLE}'] != 0)]


# Exclude observations outside the bounds of expenditure percentile.
shs_data['x'] = shs_data[[f's{i}' for i in SHAREABLE] 
                         + [f's{NONSHAREABLE}']].sum(axis=1)
selected = (
    (shs_data['x'] > np.quantile(shs_data['x'], BOUND[0])) &
    (shs_data['x'] < np.quantile(shs_data['x'], BOUND[1]))
    )
shs_data = shs_data[selected].reset_index(drop=True)


# Setup groups (province-year-type).
g = pd.DataFrame(
    (shs_data[
        [f'z{i}' for i in range(38, 51)]
        ].values[:, None] 
     * shs_data[
         [f'z{j}' for j in range(29, 38)]
         ].values[..., None]).reshape(shs_data.shape[0], -1)
    )
NAME_g = [f'g{i+1}' for i in range(np.shape(g)[1])]
g.columns = NAME_g

sm = np.where((shs_data['z23'] == 1) & (shs_data['z3'] == 0), 1, 0)
sf = np.where((shs_data['z23'] == 1) & (shs_data['z3'] == 1), 1, 0)
t = np.vstack((sm, sf)).T

for i in TYPE:
    t = np.hstack((t, np.where(shs_data[f'z{i}'] == 1, 1, 0)[:, None]))

gt = pd.DataFrame(
    (g.values[:, None] * t[..., None]).reshape(shs_data.shape[0], -1)
    )
gt_excluded = [i for i in range(gt.shape[1]) if gt[i].sum() == 1]
idx_excluded = [
    gt.index[
        gt[gt_excluded[i]] == 1
        ].tolist()[0] for i in range(len(gt_excluded))
        ]

shs_data = shs_data.drop(idx_excluded).reset_index(drop=True)
g = g.drop(idx_excluded).reset_index(drop=True)
t = np.delete(t, idx_excluded, 0)
NAME_t = ['sm', 'sf'] + [f'h{i+1}' for i in range(len(TYPE))]
t_single = np.hstack(((t[:, 0] + t[:, 1])[:, None], t[:, 2:]))
NAME_t_single = ['s'] + [f'h{i+1}' for i in range(len(TYPE))]

gt = scipy.sparse.csr_array(
    gt.drop(idx_excluded).reset_index(drop=True).values
    ).nonzero()
clusters = np.array(
    [[
        int(g.columns[g.iloc[i].isin([1.0])][0].replace('g', ''))
        ] for i in range(g.shape[0])]
    )


# Create variables.
def demean(var, gt):
    """Difference-out the group mean from each observation.

    Parameters:
    -----------
    var: 
        An N*P NumPy array of the sample where N is the number of observations  
        and P is the number of variables.
    gt:
        A sparse matrix of group indicators generated by the function 
        scipy.sparse.csr_array(matrix). The input matrix must be an N*G matrix 
        where N is the number of observations and G is the number of groups.

    Outputs:
    --------
    var_demean: 
        An N*P NumPy array of the demeaned sample where N is the number of      
        observations and P is the number of variables.
    """
    var_demean = np.zeros((var.shape[0], var.shape[1]))

    for i in np.unique(gt[1]):
        index = gt[0][np.where(gt[1] == i)]
        var_demean[index, :] = var[index, :] - var[index, :].mean(axis=0)
    
    return var_demean

y = np.log(shs_data['x'].values) - shs_data[f'p{NONSHAREABLE}'].values
r = ((shs_data[[f'p{i}' for i in SHAREABLE]]).values 
     - shs_data[f'p{NONSHAREABLE}'].values[:, None])
er = np.exp(r)
z = shs_data[[f'z{i}' for i in DEMOG]].values
w = ((shs_data[[f's{i}' for i in SHAREABLE]]).values 
     / shs_data['x'].values[:, None])
w_non = shs_data[f's{NONSHAREABLE}'].values / shs_data['x'].values
NAME_w = [f'w{i}' for i in SHAREABLE]
wyz = np.hstack((w, y[:, None], z))
wyz_demean = demean(wyz, gt)
ery = er * wyz_demean[:, len(SHAREABLE)][:, None]

# Group variables by types.
eryt = pd.DataFrame(
    (ery[:, None] * t_single[..., None]).reshape(t_single.shape[0], -1), 
    columns=[f'er{SHAREABLE[i]}y_{tt}' 
             for tt in NAME_t_single 
             for i in range(len(SHAREABLE))]
    )
NAME_eryt = eryt.columns.tolist()
eryt_test = pd.DataFrame(
    (ery[:, None] * t[..., None]).reshape(t.shape[0], -1), 
    columns=[f'er{SHAREABLE[i]}y_{tt}_test'
             for tt in NAME_t 
             for i in range(len(SHAREABLE))]
    )
NAME_eryt_test = eryt_test.columns.tolist()
zt = pd.DataFrame(
    (wyz_demean[:, (len(SHAREABLE)+1):][:, None]
     * t[..., None]).reshape(t.shape[0], -1), 
     columns=[f'z{DEMOG[i]}_{tt}'
              for tt in NAME_t 
              for i in range(len(DEMOG))]
    )
NAME_zt = zt.columns.tolist()


# Combine variables into a single dataframe.
shs_data = pd.concat([
    pd.DataFrame(
        wyz_demean[:, range(len(SHAREABLE))], 
        columns = NAME_w
        ), 
    eryt, 
    eryt_test, 
    zt
    ], axis=1)
w_all = np.hstack((w, w_non[:, None]))


# Summary statistics
summary_stat = pd.DataFrame(
    index=(NAME_w 
           + [f'w{NONSHAREABLE}'] 
           + [f'r{i}' for i in SHAREABLE] 
           + ['y'] 
           + [f'z{DEMOG[i]}' for i in range(len(DEMOG))] 
           + ['nobs'])
    )

for i in range(t.shape[1]):
    idx = np.where(t[:, i] == 1)[0]
    nobs = np.array([t[:, i].sum()])
    mean_r = r[idx, :].mean(axis=0)
    sd_r = r[idx, :].std(axis=0)
    mean_y = np.array([y[idx].mean()])
    sd_y = np.array([y[idx].std()])
    mean_w = w_all[idx, :].mean(axis=0)
    sd_w = w_all[idx, :].std(axis=0)
    mean_z = z[idx, :].mean(axis=0)
    sd_z = z[idx, :].std(axis=0)
    new_stat = pd.DataFrame(
        np.vstack(
            (np.concatenate([mean_w, mean_r, mean_y, mean_z, nobs]), 
             np.concatenate([sd_w, sd_r, sd_y, sd_z, np.array([np.nan])]))
            ).T, 
        columns=[f'{NAME_t[i]}.mean', f'{NAME_t[i]}.sd'], 
        index=(NAME_w 
               + [f'w{NONSHAREABLE}'] 
               + [f'r{i}' for i in SHAREABLE] 
               + ['y'] 
               + [f'z{DEMOG[i]}' for i in range(len(DEMOG))] 
               + ['nobs'])
        )
    summary_stat = pd.concat([summary_stat, new_stat], axis=1)

del ery, eryt, eryt_test, wyz_demean, w_non, sm, sf, zt, selected


# Estimate reduced-form equations (mean Barten scales).
mean_eqs = OrderedDict()

for i in range(len(SHAREABLE)):
    dep = shs_data[NAME_w[i]]
    exog = shs_data[
        [col for col in NAME_eryt if f"er{SHAREABLE[i]}y_" in col] 
        + NAME_zt
        ]
    mean_eqs[f'eq{i+1}'] = {"dependent":dep, "exog":exog}

mean_est = SUR(mean_eqs).fit(cov_type="clustered",
                             clusters=clusters,
                             debiased=True)

del mean_eqs, dep, exog


# Prepare data for the estimation of Barten scale variances.
ery = er * y[:, None]
eryt = (ery[:,None] * t_single[...,None]).reshape(t_single.shape[0], -1)
zt = (z[:,None] * t[...,None]).reshape(t.shape[0], -1)
res = np.zeros((np.shape(shs_data)[0], len(SHAREABLE)))

for i in range(len(SHAREABLE)):
    fres = w[:, i] - (
        np.hstack((eryt[:, np.arange(i, len(NAME_eryt), len(SHAREABLE))], zt))
        @ mean_est.params.filter(like=f'eq{i+1}').values[:, None]
        ).T.flatten()
    for j in np.unique(gt[1]):
        index = gt[0][np.where(gt[1] == j)]
        res[index, i] = fres[index] - np.mean(fres[index])

bery = ery * mean_est.params.filter(like=f'y_s').values
bery_crossprod = pd.DataFrame(index=shs_data.index)
bery_crosssum = pd.DataFrame(index=shs_data.index)
res_crossprod = pd.DataFrame(index=shs_data.index)

for i in range(len(SHAREABLE)):
    for j in range(i, len(SHAREABLE)):
        res_crossprod = pd.concat(
            [res_crossprod, 
             pd.DataFrame(
                 res[:, i] * res[:, j], 
                 columns=[f'res{SHAREABLE[i]}.{SHAREABLE[j]}']
                 )], 
            axis=1
            )
        bery_crossprod = pd.concat(
            [bery_crossprod, 
             pd.DataFrame(
                 bery[:, i] * bery[:, j], 
                 columns=[f'ber{SHAREABLE[i]}.{SHAREABLE[j]}y_prod']
                 )], 
            axis=1
            )
        bery_crosssum = pd.concat(
            [bery_crosssum, 
             pd.DataFrame(
                 bery[:, i] + bery[:, j], 
                 columns=[f'ber{SHAREABLE[i]}.{SHAREABLE[j]}y_sum']
                 )], 
            axis=1
            )
        
NAME_res_crossprod = res_crossprod.columns
NAME_bery_crossprod = bery_crossprod.columns
NAME_bery_crosssum = bery_crosssum.columns

shs_res = pd.concat([
    pd.DataFrame(t_single[:, 1:], columns=NAME_t[2:]), 
    res_crossprod, 
    bery_crossprod, 
    g, 
    pd.DataFrame(
        (bery_crosssum.values[:,None]
         * g.values[...,None]).reshape(g.shape[0], -1),
        columns=[f'g{i+1}_{NAME_bery_crosssum[j]}'
                 for i in range(np.shape(g)[1])
                 for j in range(len(NAME_bery_crosssum))]
        )
    ], 
    axis=1)[~(t_single[:, 0] == 1)].reset_index(drop=True)

del w, y, er, ery, res, z, zt, fres, index, r, eryt, bery, g
del res_crossprod, bery_crossprod, bery_crosssum


# Estimate Barten scale variances, standard deviations, and correlations.
def cov_to_cor(cov):
    """Convert a covariance matrix to a correlation matrix.

    Parameters:
    -----------
    cov:
        A 2-d NumPy array of covariance matrix.

    Outputs:
    --------
    cor:
        A 2-d NumPy array of correlation matrix.
    """
    v = np.sqrt(np.diag(cov))
    cor = cov / np.outer(v, v)

    return cor

cov_matrix = np.empty((len(SHAREABLE), len(SHAREABLE), len(TYPE)))
cov_matrix_lrobust = np.empty((len(SHAREABLE), len(SHAREABLE), len(TYPE)))
cov_matrix_urobust = np.empty((len(SHAREABLE), len(SHAREABLE), len(TYPE)))
cov_matrix_pd = np.empty((len(SHAREABLE), len(SHAREABLE), len(TYPE)))
cov_matrix_pd_lrobust = np.empty((len(SHAREABLE), len(SHAREABLE), len(TYPE)))
cov_matrix_pd_urobust = np.empty((len(SHAREABLE), len(SHAREABLE), len(TYPE)))
cor_matrix = np.empty((len(SHAREABLE), len(SHAREABLE), len(TYPE)))

for tt in range(len(TYPE)):
    res_data = shs_res[
        shs_res[f'h{tt+1}'] == 1
        ].astype(float).reset_index(drop=True)
    
    g_included = [i for i in NAME_g if res_data[i].sum() >= 2]
    g_bery_included = [
        f'{i}_{j}' for i in g_included for j in NAME_bery_crosssum
        ]
    g_new = scipy.sparse.csr_array(res_data[NAME_g].values)
    g_new = g_new.nonzero()
    res_data = pd.DataFrame(demean(res_data.values, g_new), 
                            columns=res_data.columns)
    
    var_eqs = OrderedDict()
    var_eqs_lrobust = OrderedDict()
    var_eqs_urobust = OrderedDict()
    for i in range(len(NAME_res_crossprod)):
        dep = res_data[NAME_res_crossprod[i]]
        name = NAME_res_crossprod[i].replace("res", "ber") + 'y'
        exog = res_data[[j for j in g_bery_included if name in j] 
                        + [NAME_bery_crossprod[i]]]
        
        cv = RepeatedKFold(random_state=123)
        penalty = np.array([1.0]*(exog.shape[1]-1) + [0.0])

        lasso_model = CustomENetCV(
            cv=cv, 
            l1_ratio=1, 
            fit_intercept=False, 
            verbose=False, 
            max_iter=10000
            )
        lasso_model.fit(exog.to_numpy(), dep.to_numpy(), s=penalty)

        lasso_lrobust = CustomENet(
            lasso_model.alpha_best*0.5, 
            l1_ratio=1, 
            fit_intercept=False
            )
        lasso_lrobust.fit(exog.to_numpy(), dep.to_numpy(), s=penalty)

        lasso_urobust = CustomENet(
            lasso_model.alpha_best*2, 
            l1_ratio=1, 
            fit_intercept=False
            )
        lasso_urobust.fit(exog.to_numpy(), dep.to_numpy(), s=penalty)

        exog_selected = exog[exog.columns[np.nonzero(lasso_model.w)[0]]]
        exog_lrobust = exog[exog.columns[np.nonzero(lasso_lrobust.w)[0]]]
        exog_urobust = exog[exog.columns[np.nonzero(lasso_urobust.w)[0]]]
        var_eqs[f'eq{i+1}'] = {"dependent": dep, "exog": exog_selected}
        var_eqs_lrobust[f'eq{i+1}'] = {"dependent": dep, "exog": exog_lrobust}
        var_eqs_urobust[f'eq{i+1}'] = {"dependent": dep, "exog": exog_urobust}
    
    cov_est = SUR(var_eqs).fit(cov_type="unadjusted")
    cov_est_lrobust = SUR(var_eqs_lrobust).fit(cov_type="unadjusted")
    cov_est_urobust = SUR(var_eqs_urobust).fit(cov_type="unadjusted")

    cov = cov_est.params[cov_est.params.index.str.contains("prod")]
    cov_lrobust = cov_est_lrobust.params[
        cov_est_lrobust.params.index.str.contains("prod")
        ]
    cov_urobust = cov_est_urobust.params[
        cov_est_urobust.params.index.str.contains("prod")
        ]
    
    cov_matrix[:, :, tt][
        np.triu_indices_from(cov_matrix[:, :, tt], k=0)
        ] = cov
    cov_matrix[:, :, tt][
        np.tril_indices_from(cov_matrix[:, :, tt], k=-1)
        ] = cov_matrix[:, :, tt].T[
            np.tril_indices_from(cov_matrix[:, :, tt], k=-1)
            ]
    cov_matrix_pd[:, :, tt] = NPD.nearestPD(
        cov_matrix[:, :, tt]
        )
    
    cov_matrix_lrobust[:, :, tt][
        np.triu_indices_from(cov_matrix_lrobust[:, :, tt], k=0)
        ] = cov_lrobust
    cov_matrix_lrobust[:, :, tt][
        np.tril_indices_from(cov_matrix_lrobust[:, :, tt], k=-1)
        ] = cov_matrix_lrobust[:, :, tt].T[
            np.tril_indices_from(cov_matrix_lrobust[:, :, tt], k=-1)
            ]
    cov_matrix_pd_lrobust[:, :, tt] = NPD.nearestPD(
        cov_matrix_lrobust[:, :, tt]
        )
    
    cov_matrix_urobust[:, :, tt][
        np.triu_indices_from(cov_matrix_urobust[:, :, tt], k=0)
        ] = cov_urobust
    cov_matrix_urobust[:, :, tt][
        np.tril_indices_from(cov_matrix_urobust[:, :, tt], k=-1)
        ] = cov_matrix_urobust[:, :, tt].T[
            np.tril_indices_from(cov_matrix_urobust[:, :, tt], k=-1)
            ]
    cov_matrix_pd_urobust[:, :, tt] = NPD.nearestPD(
        cov_matrix_urobust[:, :, tt]
        )
    
    cor_matrix[:, :, tt] = cov_to_cor(cov_matrix_pd[:, :, tt])

barten_sd = pd.DataFrame(
    index=NAME_w, 
    columns=['sd.'+i for i in [f"h{i+1}" for i in range(len(TYPE))]]
    )
barten_sd_lrobust = pd.DataFrame(
    index=NAME_w, 
    columns=['lower.sd.'+i for i in [f"h{i+1}" for i in range(len(TYPE))]]
    )
barten_sd_urobust = pd.DataFrame(
    index=NAME_w, 
    columns=['upper.sd.'+i for i in [f"h{i+1}" for i in range(len(TYPE))]]
    )

for i in range(len(TYPE)):
    barten_sd[f'sd.h{i+1}'] = np.sqrt(
        np.diag(cov_matrix_pd[:, :, i])
        )
    barten_sd_lrobust[f'lower.sd.h{i+1}'] = np.sqrt(
        np.diag(cov_matrix_pd_lrobust[:, :, i])
        )
    barten_sd_urobust[f'upper.sd.h{i+1}'] = np.sqrt(
        np.diag(cov_matrix_pd_urobust[:, :, i])
        )
    

# Define Barten scale and household scale functions.
def barten(theta, num_eq):
    """Calculate Barten scale estimates from the estimates of the reduced-form 
    regressions.

    Parameters:
    -----------
    theta:
        A NumPy array of length 2*K, where K is the number of shareable goods. 
        The first K elements of the array must be the reduced-form estimates for 
        single-member households, and the last K elements must be the 
        corresponding estimates for multi-member households.
    num_eq:
        The number of equations in the regression system, aka the number of 
        shareable goods.
    
    Outputs:
    --------
    a:
        A NumPy array of Barten scale estimates.
    """
    b_s = theta[:num_eq]
    b_h = theta[num_eq:]

    a = b_h / b_s

    return a

def barten_deriv(theta, num_eq):
    """Calculate the derivatives of Barten scales from the estimates of the 
    reduced-form regressions. This is used for the delta method standard errors.

    Parameters:
    -----------
    theta:
        A NumPy array of length 2*K, where K is the number of shareable goods. 
        The first K elements of the array must be the reduced-form estimates for 
        single-member households, and the last K elements must be the 
        corresponding estimates for multi-member households.
    num_eq:
        The number of equations in the regression system, aka the number of 
        shareable goods.

    Outputs:
    --------
    a:
        A NumPy array of Barten scale derivatives.
    """
    b_s = theta[:num_eq]
    b_h = theta[num_eq:]

    dads = np.divide(-b_h, np.square(b_s))
    dadh = 1 / b_s

    return np.vstack([np.diag(dads), np.diag(dadh)])

def barten_results(
        mean_est, 
        barten=barten, 
        barten_deriv=barten_deriv, 
        TYPE=None, 
        NAME_w=None):
    """Generate dataframes for Barten scale estimates and standard errors, and 
    their covariance matrices.

    Parameters:
    -----------
    mean_est:
        Estimation results of the reduced-form equations for mean Barten scales. 
        It is genetated directly by the function linearmodels.system.SUR.fit().
    barten:
        A previously defined function that returns Barten scale estimates.
    barten_deriv:
        A previously defined function that returns Barten scale derivatives.
    TYPE:
        A tuple of indices for household types.
    NAME_w:
        A list of indices for shareable goods.

    Outputs:
    --------
    a:
        A dataframe listing all Barten scale estimates, with K rows for 
        shareable goods and T columns for household types.
    se_a:
        A dataframe listing all Barten scale standard errors, with K rows for 
        shareable goods and T columns for household types.
    vcov_a:
        A 3-d NumPy array of the covariance matrices of Barten scale estimates 
        with dimension (K, K, T), where K is the number of shareable goods and T 
        is the number of household types.
    """
    a = pd.DataFrame(index=NAME_w, 
                     columns=[f'h{i}' for i in range(len(TYPE))], 
                     dtype=float)
    se_a = pd.DataFrame(index=NAME_w, 
                        columns=[f'se.h{i}' for i in range(len(TYPE))], 
                        dtype=float)
    vcov_a = np.empty((len(NAME_w), len(NAME_w), len(TYPE)))

    for i in range(len(TYPE)):
        theta_s = mean_est.params[
            (mean_est.params.index.str.contains('s') == True) & 
            (mean_est.params.index.str.contains('z') == False)
            ]
        theta_h = mean_est.params[
            (mean_est.params.index.str.contains(f'h{i+1}') == True) & 
            (mean_est.params.index.str.contains('z') == False)
            ]
        theta = np.concatenate([theta_s.values, theta_h.values])
        vcovar = mean_est.cov.loc[
            theta_s.index.tolist()+theta_h.index.tolist(), 
            theta_s.index.tolist()+theta_h.index.tolist()
            ].values
        
        a[f'h{i}'] = barten(theta, len(NAME_w))
        se_a[f'se.h{i}'] = np.sqrt(
            (barten_deriv(theta, len(NAME_w)).T 
             @vcovar 
             @barten_deriv(theta, len(NAME_w))).diagonal()
             )
        vcov_a[:, :, i] = (barten_deriv(theta, len(NAME_w)).T 
                           @vcovar 
                           @barten_deriv(theta, len(NAME_w)))
        
    return a, se_a, vcov_a

def scale(barten, cov, cov_se, data, single_indicator, TYPE, NAME_w):
    """Generate a dataframe for the household-level scale economies index.

    Parameters:
    -----------
    barten:
        A dataframe of Barten scale estimates under different household types. 
        This is the first output of the function barten_results().
    cov:
        A 3-d NumPy array of the of the estimated random Barten scale covariance 
        matrices (the second moment of Barten scales) under different household 
        types, with dimension (K, K, T), where K is the number of shareable 
        goods and T is the number of household types.
    cov_se:
        A 3-d NumPy array of the covariance matrices of Barten scale estimates   
        under different household types. This is the third output of the 
        function barten_results().
    data:
        An N*(K+1) NumPy array of the budget shares where N is the number of 
        observations and K is the number of shareable goods. The last column 
        should be the nonshareable good.
    single_indicator:
        A NumPy array of length N indicating if an observation is single, where 
        N is the number of observations.
    TYPE:
        A tuple of indices for household types.
    NAME_w:
        A list of indices for shareable goods.

    Outputs:
    --------
    scales:
        A dataframe for the household-level scale economies indices under 
        different household types.
    """
    w_bar = data[single_indicator == 1, :].mean(axis=0)
    w_cov = pd.DataFrame(data[single_indicator == 1, :]).cov().values
    scales = pd.DataFrame(
        index=["mean", "mean_se", "sd"], 
        columns=[f"h{i+1}" for i in range(len(TYPE))]
        )
    
    for i in range(len(scales.columns)):
        barten_full = np.concatenate([barten.iloc[:, i], [1]])
        barten_cov_se_full = np.block(
            [[cov_se[:, :, i], np.zeros((len(NAME_w), 1))], 
             [np.zeros((1, len(NAME_w) + 1))]]
            )
        barten_cov_full = np.block(
            [[cov[:, :, i], np.zeros((len(NAME_w), 1))], 
             [np.zeros((1, len(NAME_w) + 1))]]
            )
        
        scales.iloc[0, i] = sum(w_bar*barten_full)
        scales.iloc[1, i] = np.sqrt(w_bar@barten_cov_se_full@w_bar)
        scales.iloc[2, i] = np.sqrt(barten_full@w_cov@barten_full 
                                    + w_bar@barten_cov_full@w_bar)
        
    return scales
    

# Print results.
print("Summary Statistics")
print(summary_stat.astype(float).round(4))

print("Barten estimates")
barten_est = barten_results(mean_est, TYPE=TYPE, NAME_w=NAME_w)
print(barten_est[0].astype(float).round(4))
print(barten_est[1].astype(float).round(4))
print(barten_sd.astype(float).round(4))

print("LASSO robustness check, 0.5*penalty")
print(barten_sd_lrobust.astype(float).round(4))

print("LASSO robustness check, 2*penalty")
print(barten_sd_urobust.astype(float).round(4))

print("Covariance matrix, unadjusted")
np.set_printoptions(precision=4, suppress=True)
for i in range(len(TYPE)):
    print(f'h{i+1}')
    print(cov_matrix[:, :, i])

print("Covariance matrix, adjusted")
for i in range(len(TYPE)):
    print(f'h{i+1}')
    print(cov_matrix_pd[:, :, i])

print("Correlation matrix")
for i in range(len(TYPE)):
    print(f'h{i+1}')
    print(cor_matrix[:, :, i])

print("Scale economy index, singles")
scale_s = scale(barten_est[0], 
                cov_matrix_pd, 
                barten_est[2], 
                w_all, 
                t_single[:, 0], 
                TYPE, 
                NAME_w)
print(scale_s.astype(float).round(4))

print("Scale economy index, single males")
scale_sm = scale(barten_est[0], 
                 cov_matrix_pd, 
                 barten_est[2], 
                 w_all, 
                 t[:, 0], 
                 TYPE, 
                 NAME_w)
print(scale_sm.astype(float).round(4))

print("Scale economy index, single females")
scale_sf = scale(barten_est[0], 
                 cov_matrix_pd, 
                 barten_est[2], 
                 w_all, 
                 t[:, 1], 
                 TYPE, 
                 NAME_w)
print(scale_sf.astype(float).round(4))

del shs_res, res_data, dep, exog, exog_selected, exog_lrobust, exog_urobust
    

# Test whether Barten scales equal their upper & lower bounds.
lbound = pd.DataFrame(
    index=NAME_w+['s', 'sm', 'sf'], 
    columns=[f'h{i}.lower' for i in range(len(TYPE))], 
    dtype=float
    )
ubound = pd.DataFrame(
    index=NAME_w+['s', 'sm', 'sf'], 
    columns=[f'h{i}.upper' for i in range(len(TYPE))], 
    dtype=float
    )
lbound_pvalue = pd.DataFrame(
    index=NAME_w+['s', 'sm', 'sf'], 
    columns=[f'h{i}.lower.pvalue' for i in range(len(TYPE))], 
    dtype=float
    )
ubound_pvalue = pd.DataFrame(
    index=NAME_w+['s', 'sm', 'sf'], 
    columns=[f'h{i}.upper.pvalue' for i in range(len(TYPE))], 
    dtype=float
    )

for i in range(len(TYPE)):
    lower = ((barten_est[0][f'h{i}'].values - (1/NUM_PEOPLE[i]))
             /barten_est[1][f'se.h{i}']).values
    lower_scale_s = ((scale_s.iloc[0][ f'h{i+1}'] - (1/NUM_PEOPLE[i]))
                     /scale_s.iloc[1][ f'h{i+1}'])
    lower_scale_sm = ((scale_sm.iloc[0][ f'h{i+1}'] - (1/NUM_PEOPLE[i]))
                      /scale_sm.iloc[1][ f'h{i+1}'])
    lower_scale_sf = ((scale_sf.iloc[0][ f'h{i+1}'] - (1/NUM_PEOPLE[i]))
                      /scale_sf.iloc[1][ f'h{i+1}'])
    
    upper = ((barten_est[0][f'h{i}'].values - 1)
             /barten_est[1][f'se.h{i}']).values
    upper_scale_s = ((scale_s.iloc[0][ f'h{i+1}'] - 1)
                     /scale_s.iloc[1][ f'h{i+1}'])
    upper_scale_sm = ((scale_s.iloc[0][ f'h{i+1}'] - 1)
                      /scale_sm.iloc[1][ f'h{i+1}'])
    upper_scale_sf = ((scale_s.iloc[0][ f'h{i+1}'] - 1)
                      /scale_sf.iloc[1][ f'h{i+1}'])
    
    lbound[f'h{i}.lower'] = (lower.tolist() 
                             + [lower_scale_s, lower_scale_sm, lower_scale_sf])
    ubound[f'h{i}.upper'] = (upper.tolist() 
                             + [upper_scale_s, upper_scale_sm, upper_scale_sf])
    
    lbound_pvalue[f'h{i}.lower.pvalue'] = scipy.stats.norm.sf(abs(
        np.array(lower.tolist() + [lower_scale_s, 
                                   lower_scale_sm, 
                                   lower_scale_sf])
        ))
    ubound_pvalue[f'h{i}.upper.pvalue'] = scipy.stats.norm.sf(abs(
        np.array(upper.tolist() + [upper_scale_s, 
                                   upper_scale_sm, 
                                   upper_scale_sf])
        ))
    
print(lbound.astype(float).round(4))
print(lbound_pvalue.astype(float).round(4))
print(ubound.astype(float).round(4))
print(ubound_pvalue.astype(float).round(4))


# Test whether the Engel curves for single males and females are identical.
test_eqs = OrderedDict()
for i in range(len(SHAREABLE)):
    dependent = shs_data[NAME_w[i]]
    exog = shs_data[
        [j for j in NAME_eryt_test if f"er{SHAREABLE[i]}y_" in j] 
        + NAME_zt
        ]
    test_eqs[f'eq{i+1}'] = {"dependent": dependent, "exog": exog}
SURest_test = SUR(test_eqs).fit(cov_type="clustered", 
                                clusters = clusters, 
                                debiased=True)

sim_test = pd.DataFrame(
    index=["sm", "sm_se", "sf", "sf_se", "chisq", "p-value"], 
    columns=[f"b{SHAREABLE[i]}" for i in range(len(SHAREABLE))] + ["joint"]
    )
hypotheses_sim = [
    f'eq{i+1}_er{SHAREABLE[i]}y_sm_test - eq{i+1}_er{SHAREABLE[i]}y_sf_test=0'
    for i in range(len(SHAREABLE))
    ]

for i in range(len(SHAREABLE)):
    name = [f'eq{i+1}_er{SHAREABLE[i]}y_sm_test',
            f'eq{i+1}_er{SHAREABLE[i]}y_sf_test']
    
    sim_test.loc[["sm", "sf"], 
                 f"b{SHAREABLE[i]}"] = SURest_test.params[name].values
    sim_test.loc[["sm_se", "sf_se"], 
                 f"b{SHAREABLE[i]}"] = SURest_test.std_errors[name].values
    wald = linearmodels.shared.hypotheses.quadratic_form_test(
        params=SURest_test.params, 
        cov=SURest_test.cov, 
        formula=hypotheses_sim[i]
        )
    sim_test.loc[["chisq", "p-value"], 
                 f"b{SHAREABLE[i]}"] = [wald.stat, wald.pval]
    
wald = linearmodels.shared.hypotheses.quadratic_form_test(
    params=SURest_test.params, 
    cov=SURest_test.cov, 
    formula=hypotheses_sim
    )
sim_test.loc[["chisq", "p-value"], 'joint'] = [wald.stat, wald.pval]

print("Similarity test")
print(sim_test.astype(float).round(4))

del test_eqs, dependent, exog


# Test whether the model is identified.
id_test = pd.DataFrame(
    index=["est", "se", "chisq", "p-value"], 
    columns=[f"b{SHAREABLE[i]}" for i in range(len(SHAREABLE))]
    )
hypotheses_id = [f'eq{i+1}_er{SHAREABLE[i]}y_s=0' 
                 for i in range(len(SHAREABLE))]

id_test.loc["est", :] = mean_est.params[
    mean_est.params.index.str.contains("y_s")
    ].values
id_test.loc["se", :] = mean_est.std_errors[
    mean_est.std_errors.index.str.contains("y_s")
    ].values

for i in range(len(SHAREABLE)):
    wald = linearmodels.shared.hypotheses.quadratic_form_test(
        params=mean_est.params, 
        cov=mean_est.cov, 
        formula=hypotheses_id[i]
        )
    id_test.loc[["chisq", "p-value"], 
                f"b{SHAREABLE[i]}"] = [wald.stat, wald.pval]
    
wald = linearmodels.shared.hypotheses.quadratic_form_test(
    params=mean_est.params, 
    cov=mean_est.cov, 
    formula=hypotheses_id
    )
id_test['joint'] = [np.nan, np.nan, wald.stat, wald.pval]

print("Identification test")
print(id_test.astype(float).round(4))